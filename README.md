Egoshots dataset and Semantic Fidelity metric
=====
This repo contains code for our paper "Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models" accepted at ICLR-2020, MACHINE LEARNING IN REAL LIFE (ML-IRL) workshop.
## Dataset
Egoshots consists of real-life ego-vision images captioned using state of the art image captioning models, and aims at evaluating the robustness, diversity, and sensitivity of these models, as well as providing a real life-logging setting on-the-wild dataset that can aid the task of evaluating real settings. It consists of images from two computer science interns
for 1 month each. Egoshots images are availaible to download at "link" with corresponding captions "link".
## Captioning Egoshots
Unlabelled images of the Egoshots dataset are captioned by exploiting different image captioning models. We limit our work to three models namely:- 
1. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
2. [nocaps: novel object captioning at scale](https://arxiv.org/pdf/1812.08658.pdf)
3. [Decoupled Novel Object Captioner](https://arxiv.org/pdf/1804.03803.pdf)
# Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
